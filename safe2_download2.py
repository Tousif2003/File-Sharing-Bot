import asyncio
import os
from pyrogram import Client
from pyrogram.errors import FloodWait, RPCError
from dotenv import load_dotenv
from contextlib import asynccontextmanager

# ================= Load Environment Variables =================
load_dotenv("config.env")

# ================= Multi-client Initialization =================
multi_clients = []

# ----- API_ID/API_HASH clients -----
i = 1
while True:
    api_id = os.getenv(f"API_ID_{i}")
    api_hash = os.getenv(f"API_HASH_{i}")
    if not api_id or not api_hash:
        break
    multi_clients.append(Client(f"session_api_{i}", api_id=int(api_id), api_hash=api_hash))
    i += 1

# ----- Bot token clients -----
i = 1
while True:
    token = os.getenv(f"MULTI_TOKEN{i}")
    if not token:
        break
    multi_clients.append(Client(f"session_bot_{i}", bot_token=token))
    i += 1

if not multi_clients:
    raise Exception("‚ùå No clients found! Add API_ID/API_HASH or MULTI_TOKENs in config.env")

# ================= Download Configuration =================
CHUNK_SIZE_SMALL = 600 * 1024      # 600 KB for large files
CHUNK_SIZE_LARGE = 1024 * 1024     # 1 MB for small files
TIMEOUT = 25                        # per chunk timeout in seconds
MAX_RETRIES = 6                      # maximum retries per chunk
SAVE_DIR = "/tmp"                    # fallback storage
MAX_CONCURRENT_DOWNLOADS = 6         # safe for free tiers

DOWNLOAD_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)

# ======= Workload tracking for multi-client =======
work_loads = {idx: 0 for idx in range(len(multi_clients))}

@asynccontextmanager
async def track_workload(client_index):
    work_loads[client_index] += 1
    try:
        yield
    finally:
        work_loads[client_index] -= 1

def get_optimal_client():
    """Select the client with least active downloads."""
    idx = min(work_loads, key=work_loads.get)
    return idx, multi_clients[idx]

# ======= Cleanup File Function =======
def cleanup_file(file_path):
    """Delete temporary file after usage."""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"üóëÔ∏è  Cleanup done: {file_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error deleting {file_path}: {e}")

# ======= Safe Download with Resume + RAM Logic =======
async def safe_download(client: Client, message):
    """Download Telegram file safely with progress, resume & cleanup."""
    file_name = getattr(message.document or message.video or message.audio, "file_name", "file")
    file_size = getattr(message.document or message.video or message.audio, "file_size", 0)

    # Sanitize filename
    file_name = "".join(c for c in file_name if c.isalnum() or c in (" ", ".", "_", "-"))
    file_path = os.path.join(SAVE_DIR, file_name)
    os.makedirs(SAVE_DIR, exist_ok=True)

    # In-memory vs Disk download logic (1GB threshold)
    if file_size >= 1024 * 1024 * 1024:  # 1GB+
        in_memory = True
        print("‚ö° Using in-memory (RAM) download for fast performance.")
    else:
        in_memory = False
        print("üíæ Using disk (/tmp) download for safety.")

    # Resume offset logic
    offset = 0
    if os.path.exists(file_path):
        offset = os.path.getsize(file_path)
        if offset < file_size:
            print(f"üîÑ Resuming from {offset / 1024 / 1024:.2f} MB...")
        else:
            print("‚úÖ File already complete, skipping download.")
            return file_path

    chunk_size = CHUNK_SIZE_SMALL if file_size > 200 * 1024 * 1024 else CHUNK_SIZE_LARGE
    retries = 0

    print(f"üöÄ Downloading '{file_name}' ({file_size / 1024 / 1024:.2f} MB) [chunk {chunk_size // 1024} KB]")

    async with DOWNLOAD_SEMAPHORE:
        try:
            mode = "r+b" if os.path.exists(file_path) else "wb"
            with open(file_path, mode) as f:
                if offset:
                    f.seek(offset)
                while offset < file_size:
                    try:
                        # Download chunk
                        data = await asyncio.wait_for(
                            client.download_media(
                                message,
                                in_memory=in_memory,
                                offset=offset,
                                limit=chunk_size
                            ),
                            timeout=TIMEOUT
                        )

                        if not data:
                            if offset >= file_size:
                                break
                            raise Exception("Empty chunk received")

                        # Write data
                        f.write(data)
                        offset += len(data)
                        retries = 0

                        # Progress
                        progress = (offset / file_size) * 100 if file_size else 0
                        print(f"‚¨áÔ∏è {progress:.2f}% ({offset / 1024 / 1024:.2f}/{file_size / 1024 / 1024:.2f} MB)", end="\r")

                    except asyncio.TimeoutError:
                        retries += 1
                        wait_time = 2 * retries
                        print(f"‚ö†Ô∏è Timeout at {offset}, retry {retries}/{MAX_RETRIES} after {wait_time}s")
                        if retries >= MAX_RETRIES:
                            print("‚ùå Aborting download due to repeated timeouts.")
                            break
                        await asyncio.sleep(wait_time)

                    except FloodWait as e:
                        print(f"‚è≥ FloodWait {e.value}s, sleeping...")
                        await asyncio.sleep(e.value + 1)

                    except RPCError as e:
                        retries += 1
                        wait_time = 3 * retries
                        print(f"‚ùå RPCError {type(e).__name__}, retry {retries}/{MAX_RETRIES}, wait {wait_time}s")
                        if retries >= MAX_RETRIES:
                            print("‚ùå Aborting after repeated RPC errors.")
                            break
                        await asyncio.sleep(wait_time)

                    except Exception as e:
                        retries += 1
                        wait_time = 3 * retries
                        print(f"‚ö†Ô∏è Unknown error: {e}, retry {retries}/{MAX_RETRIES}, wait {wait_time}s")
                        if retries >= MAX_RETRIES:
                            print("‚ùå Stopping after repeated unknown errors")
                            break
                        await asyncio.sleep(wait_time)

        finally:
            # Cleanup only if incomplete
            if offset < file_size:
                print(f"üßπ Partial file cleanup: {file_path}")
                cleanup_file(file_path)

    print(f"\n‚úÖ Download completed or ended: {file_name}")
    return file_path

# ======= Multi-client download wrapper =======
async def download_file(message):
    client_index, client = get_optimal_client()
    print(f"üîπ Using client {client_index}")
    async with track_workload(client_index):
        return await safe_download(client, message)

# ======= Connection Watchdog =======
async def connection_watchdog(client, name):
    while True:
        await asyncio.sleep(300)
        try:
            await client.get_me()
        except Exception:
            print(f"üîÅ Connection lost for {name}, restarting...")
            await client.stop()
            await asyncio.sleep(5)
            await client.start()

# ======= Main Bot Starter =======
async def main():
    for i, client in enumerate(multi_clients):
        await client.start()
        print(f"‚úÖ Client {i} started: {client.session_name}")
        asyncio.create_task(connection_watchdog(client, client.session_name))

    print("‚ö° ThunderBot Multi-Client Safe Downloader started")
    await asyncio.get_event_loop().create_future()  # keep bot alive

if __name__ == "__main__":
    asyncio.run(main())
