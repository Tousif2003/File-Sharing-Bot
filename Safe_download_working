import asyncio
import os
import time
from pyrogram import Client
from pyrogram.errors import FloodWait, RPCError
from dotenv import load_dotenv
from contextlib import asynccontextmanager

# ==================== Load Environment ====================
load_dotenv("config.env")

CHUNK_SIZE = 1024 * 1024          # 1 MB chunk
TIMEOUT = 30                       # per chunk timeout in seconds
MAX_RETRIES = 6                    # retries per chunk
SAVE_DIR = "/tmp"                  # temporary storage
MAX_CONCURRENT_DOWNLOADS = 5       # number of files downloading concurrently

DOWNLOAD_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)

# ==================== Multi-client Initialization ====================
multi_clients = []

# API_ID / API_HASH clients
i = 1
while True:
    api_id = os.getenv(f"API_ID_{i}")
    api_hash = os.getenv(f"API_HASH_{i}")
    if not api_id or not api_hash:
        break
    multi_clients.append(Client(f"session_api_{i}", api_id=int(api_id), api_hash=api_hash))
    i += 1

# MULTI_TOKEN bots
i = 1
while True:
    token = os.getenv(f"MULTI_TOKEN{i}")
    if not token:
        break
    multi_clients.append(Client(f"session_bot_{i}", bot_token=token))
    i += 1

if not multi_clients:
    raise Exception("‚ùå No clients found in config.env!")

# ==================== Workload Tracking ====================
work_loads = {idx: 0 for idx in range(len(multi_clients))}

@asynccontextmanager
async def track_workload(client_index):
    work_loads[client_index] += 1
    try:
        yield
    finally:
        work_loads[client_index] -= 1

def get_optimal_client():
    idx = min(work_loads, key=work_loads.get)
    return idx, multi_clients[idx]

# ==================== Safe Chunked Download ====================
async def safe_download(client: Client, message):
    """Download file safely with multi-token rotation and 1 MB chunks."""
    file_obj = getattr(message, "document", None) or getattr(message, "video", None) or getattr(message, "audio", None)
    file_name = getattr(file_obj, "file_name", "file")
    file_size = getattr(file_obj, "file_size", 0)

    # Sanitize filename
    file_name = "".join(c for c in file_name if c.isalnum() or c in (" ", ".", "_", "-")).strip()
    file_path = os.path.join(SAVE_DIR, file_name)
    os.makedirs(SAVE_DIR, exist_ok=True)

    offset = 0
    retries = 0

    print(f"üì• Starting download: {file_path} | Chunk: {CHUNK_SIZE//1024} KB | Size: {file_size/1024/1024:.2f} MB")

    async with DOWNLOAD_SEMAPHORE:
        mode = "r+b" if os.path.exists(file_path) else "wb"
        with open(file_path, mode) as f:
            if os.path.exists(file_path):
                offset = os.path.getsize(file_path)
                if offset >= file_size:
                    print(f"‚úÖ Already downloaded: {file_name}")
                    return file_path
                f.seek(offset)
                print(f"üîÑ Resuming from {offset/1024/1024:.2f} MB")

            while offset < file_size:
                try:
                    start_time = time.time()
                    data = await asyncio.wait_for(
                        client.download_media(message, in_memory=True, offset=offset, limit=CHUNK_SIZE),
                        timeout=TIMEOUT
                    )
                    if not data:
                        break
                    f.write(data)
                    offset += len(data)
                    retries = 0

                    elapsed = max(1e-6, time.time() - start_time)
                    speed_mb_s = (len(data)/1024/1024)/elapsed
                    progress = (offset/file_size)*100 if file_size else 0
                    print(f"‚¨áÔ∏è {progress:.2f}% | {speed_mb_s:.2f} MB/s | Offset: {offset/1024/1024:.2f} MB", end="\r")

                except asyncio.TimeoutError:
                    retries += 1
                    print(f"\n‚ö†Ô∏è Timeout at {offset}, retry {retries}/{MAX_RETRIES}")
                    if retries >= MAX_RETRIES:
                        print("‚ùå Aborting due to repeated timeouts")
                        break
                    await asyncio.sleep(2)

                except FloodWait as e:
                    print(f"\n‚è≥ FloodWait {e.value}s, sleeping...")
                    await asyncio.sleep(e.value + 1)

                except RPCError as e:
                    retries += 1
                    print(f"\n‚ùå RPCError {type(e).__name__}: {e}, retry {retries}/{MAX_RETRIES}")
                    if retries >= MAX_RETRIES:
                        break
                    await asyncio.sleep(3)

                except Exception as e:
                    retries += 1
                    print(f"\n‚ö†Ô∏è Unknown error: {e}, retry {retries}/{MAX_RETRIES}")
                    if retries >= MAX_RETRIES:
                        break
                    await asyncio.sleep(3)

    print(f"\n‚úÖ Download finished: {file_name}")
    return file_path

# ==================== DC-aware wrapper ====================
async def safe_download_dc(client: Client, message):
    """Handle Telegram DC migrations automatically using RPCError."""
    retries = 0
    while True:
        try:
            return await safe_download(client, message)
        except RPCError as e:
            # Agar DC migrate error aaya to print kar ke raise karo
            if "MIGRATE" in str(e).upper() or "PHONE_MIGRATE" in str(e).upper() or "NETWORK_MIGRATE" in str(e).upper():
                dc_id = getattr(e, "new_dc", None) or getattr(e, "dc_id", None)
                print(f"üîÑ DC Migration detected! Redirecting to DC {dc_id}...")

                await client.stop()
                temp_client = Client(
                    f"{client.session_name}_dc{dc_id}",
                    api_id=client.api_id,
                    api_hash=client.api_hash,
                    workdir="/tmp"
                )
                await temp_client.start()
                client = temp_client
                print(f"‚úÖ Reconnected to DC {dc_id}, resuming download...")
                retries += 1
                if retries >= MAX_RETRIES:
                    raise Exception("‚ùå Too many DC migrations, aborting")
            else:
                raise e

# ==================== Multi-client wrapper ====================
async def download_file(message):
    client_index, client = get_optimal_client()
    async with track_workload(client_index):
        return await safe_download_dc(client, message)

# ==================== Multi-file Concurrent Download ====================
async def download_multiple(messages):
    """Download multiple files concurrently (up to 5 at a time) with stable speed."""
    tasks = [asyncio.create_task(download_file(msg)) for msg in messages]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results

# ==================== Connection Watchdog ====================
async def connection_watchdog(client, name):
    while True:
        await asyncio.sleep(300)
        try:
            await client.get_me()
        except Exception:
            print(f"üîÅ Connection lost for {name}, restarting...")
            try: await client.stop()
            except Exception: pass
            await asyncio.sleep(5)
            try: await client.start()
            except Exception as e: print(f"‚ùå Failed to restart {name}: {e}")

# ==================== Main ====================
async def main():
    # Start all clients
    for i, client in enumerate(multi_clients):
        await client.start()
        print(f"‚úÖ Client {i} started: {client.session_name}")
        asyncio.create_task(connection_watchdog(client, client.session_name))

    print("‚ö° ThunderBot Multi-File Stable Downloader started")
    print("‚úÖ Concurrent download ready at 5‚Äì10 MB/s per file, up to 5 files at a time")

    # Keep alive
    await asyncio.get_event_loop().create_future()

if __name__ == "__main__":
    asyncio.run(main())
